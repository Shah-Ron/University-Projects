### AIM 

The objective of this project was to collect data from a specified website, clean and organize the data, create metadata, convert the data into the SDMX format, and provide the processed data to the client.

### Explaination:

<b>scrapeData.py</b>:  

This Python script automates the process of scraping data from a designated website. The data scraping is scheduled to run quarterly to ensure the latest data is captured. Once collected, the data is stored in an Excel file, and the filename is saved in a text file for reference.

<b>cleaning.r</b>

This R script is designed to clean the acquired data. R was chosen for its powerful data visualization and manipulation capabilities within RStudio. The script handles missing data by setting any 'Not Available' fields to zero. After cleaning, the data is saved as CSV files.

<b>dataFlask.py</b>

This Python script uses Flask to create an API for accessing the data and metadata. It provides the data in JSON format upon request.

<b>metaData.json</b>

This file contains metadata about the tables, their dimensions, and the respective observations.

<b>requirements.txt</b>

This file lists all the necessary Python and R packages required for the project.

<b>Directory Structure</b>

- Download: This directory stores the quarterly statistics files and the text file containing the filename.

- Cleaned: This directory contains the cleaned CSV files generated by cleaning.r.

### Process:

1. Data Scraping: The scrapeData.py script runs first, scraping the quarterly prison statistics and saving them in the 'Download' folder. The script also generates a filename.txt file with the name of the downloaded file to facilitate easy access during the cleaning process.
2. Data Cleaning: Next, the cleaning.r script runs to clean the scraped data and convert it into a more usable format, saving the output as CSV files in the 'Cleaned' folder.
3. API Creation: Finally, the dataFlask.py script is activated to provide access to the cleaned data and metadata through a Flask API, returning responses in JSON format.

### Deployment

All scripts and necessary files are deployed on an EC2 instance in AWS. The required packages are installed, and cron jobs are set up to automate the process every three months.

### Cron Jobs

The cron jobs are as below

```sh
0 0 1 */3 * ~/university-projects/Data Engineering/Individual/scrapeData.py
5 0 1 */3 * ~/university-projects/Data Engineering/Individual/cleaning.r
6 0 1 */3 * ~/university-projects/Data Engineering/Individual/dataFlaskAPI.py
```

These jobs are scheduled to run quarterly, with the data scraping script executing at 12:00 am, followed by the cleaning script at 12:05 am, and the API script at 12:06 am. This ensures that the latest data is always processed and made available efficiently.
